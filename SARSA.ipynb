{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1353b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time\n",
    "from typing import Iterable, List, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3207d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action indices\n",
    "UP, RIGHT, DOWN, LEFT = 0, 1, 2, 3\n",
    "\n",
    "class GridworldEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    4-action gridworld for tabular RL\n",
    "    - Observation space: Discrete(n_rows * n_cols), state index = r * n_cols + c\n",
    "    - Action space: Discrete(4) with actions {UP, RIGHT, DOWN, LEFT}\n",
    "    - Reward: +1.0 at terminal; -0.01 per step otherwise (time penalty).\n",
    "    - Episode ends when the agent reaches the terminal cell (goal).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_rows=4, n_cols=4, terminal=(None, None), render_mode=None):\n",
    "        super().__init__()\n",
    "        self.n_rows, self.n_cols = n_rows, n_cols # grid shape\n",
    "\n",
    "        # Creating spaces\n",
    "        self.observation_space = spaces.Discrete(n_rows * n_cols)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Terminal (goal) state: bottom right\n",
    "        tr, tc = terminal\n",
    "        if tr is None or tc is None:\n",
    "            tr, tc = n_rows - 1, n_cols - 1\n",
    "        self.terminal_s = tr * n_cols + tc\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Start a new episode and return agent to starting state\"\"\"\n",
    "        \n",
    "        super().reset(seed=seed)\n",
    "        self._state = 0 # starting state (top left)\n",
    "        return self._state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time-step in the Gridworld based on the given action.\n",
    "\n",
    "        Returns a 5-tuple: (next_state, reward, terminated, truncated)\n",
    "        - The agent moves one cell in the chosen direction (up, right, down, left).\n",
    "        - If the move would hit a wall, the agent stays in the same cell.\n",
    "        - Each step gives a small negative reward (-0.01) to encourage faster solutions.\n",
    "        - Reaching the goal (terminal cell) gives +1.0 and ends the episode.\n",
    "        \"\"\"\n",
    "\n",
    "        # Decode the current state index into (row, col) coordinates\n",
    "        r, c = divmod(self._state, self.n_cols)\n",
    "\n",
    "        # Define how each action changes the agent's position\n",
    "        dr, dc = {\n",
    "        UP:    (-1, 0),  # move up decreases the row index\n",
    "        RIGHT: (0, 1),   # move right increases the column index\n",
    "        DOWN:  (1, 0),   # move down increases the row index\n",
    "        LEFT:  (0, -1),  # move left decreases the column index\n",
    "        }[int(action)]\n",
    "\n",
    "        # Compute tentative next position \n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        # Boundary check: if move goes out of bounds, stay put\n",
    "        if not (0 <= nr < self.n_rows and 0 <= nc < self.n_cols):\n",
    "            nr, nc = r, c\n",
    "        \n",
    "        self._state = nr * self.n_cols + nc # converting new state to integer\n",
    "        \n",
    "        # Check for termination condition (episode ends if true)\n",
    "        terminated = (self._state == self.terminal_s)\n",
    "        reward = 1.0 if terminated else -0.01\n",
    "        return self._state, reward, terminated, False, {}\n",
    "\n",
    "    def draw(self):\n",
    "        \"\"\"Print a grid showing agent (A) and goal (T) after each action.\"\"\"\n",
    "\n",
    "        grid = []\n",
    "        for r in range(self.n_rows):\n",
    "            row = \"\"\n",
    "            for c in range(self.n_cols):\n",
    "                s = r * self.n_cols + c\n",
    "                if s == self._state:\n",
    "                    row += \"A \"\n",
    "                elif s == self.terminal_s:\n",
    "                    row += \"T \"\n",
    "                else:\n",
    "                    row += \". \"\n",
    "            grid.append(row)\n",
    "        print(\"\\n\".join(grid), \"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71791624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[ 0.48784275  0.47215995  0.77359399  0.21020667]\n",
      " [-0.00867544 -0.00847028  0.75873139  0.05229227]\n",
      " [-0.00479541 -0.00492178  0.05576012 -0.00553757]\n",
      " [-0.003726   -0.00379639  0.00231957 -0.00319894]\n",
      " [ 0.3409409   0.77750761  0.24003209  0.47110311]\n",
      " [ 0.39121299  0.39851479  0.84707998  0.50754995]\n",
      " [-0.00404992  0.06336632  0.69064499 -0.0029872 ]\n",
      " [-0.001999   -0.002809    0.70087636 -0.00210672]\n",
      " [ 0.09801819  0.75892846 -0.00490024 -0.00390587]\n",
      " [ 0.55039614  0.8845723   0.20606779  0.53378964]\n",
      " [ 0.29564707  0.96245591  0.58266552  0.55344371]\n",
      " [ 0.30613976  0.66931733  1.          0.68587067]\n",
      " [-0.002187    0.01267388 -0.003896   -0.0029079 ]\n",
      " [-0.001999    0.58510697 -0.0019     -0.001     ]\n",
      " [ 0.09350466  0.96566316  0.1468521   0.03428106]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize environment\n",
    "env = GridworldEnv(n_rows=4, n_cols=4)  # 4x4 Gridworld (simple tabular environment)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_states = env.observation_space.n   # total number of discrete states in the grid\n",
    "n_actions = env.action_space.n       # total number of possible actions (UP, RIGHT, DOWN, LEFT)\n",
    "alpha = 0.1                          # learning rate (how fast Q-values are updated)\n",
    "gamma = 0.99                         # discount factor (how much future rewards are valued)\n",
    "epsilon = 0.1                        # exploration rate (probability of taking a random action)\n",
    "episodes = 500                       # number of training episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "# Q[state, action] represents the expected value of taking a given action in a given state.\n",
    "# Start with all zeros; the agent will fill this table through experience.\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# SARSA training loop\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Choose an initial action using ε-greedy policy\n",
    "    # With probability ε, pick a random action (exploration)\n",
    "    # Otherwise, pick the action with the highest Q-value for the current state (exploitation)\n",
    "    action = np.random.choice(n_actions) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
    "\n",
    "    done = False \n",
    "\n",
    "    # Loop until the agent reaches the terminal state or is truncated\n",
    "    while not done:\n",
    "        # Take the chosen action and observe the outcome\n",
    "        # The environment returns:\n",
    "        #   next_state: the new state index\n",
    "        #   reward: numerical reward for this step\n",
    "        #   terminated: True if goal reached (episode success)\n",
    "        #   truncated: True if ended for another reason (like step limit)\n",
    "        #   _: optional info dictionary (not used here)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Choose the next action using ε-greedy policy (on-policy)\n",
    "        # SARSA is \"on-policy\" because it learns the value of the policy that is being followed\n",
    "        next_action = np.random.choice(n_actions) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
    "\n",
    "        # === SARSA update rule ===\n",
    "        # Q(s,a) ← Q(s,a) + α * [r + γ * Q(s',a') − Q(s,a)]\n",
    "        # Where:\n",
    "        #   s,a = current state and action\n",
    "        #   r = reward observed\n",
    "        #   s',a' = next state and next action\n",
    "        # This update adjusts Q-values to better predict future cumulative rewards.\n",
    "        Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "        # Move to the next state-action pair\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3647c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ ↓ ↓ ↓\n",
      "→ ↓ ↓ ↓\n",
      "→ → → ↓\n",
      "→ → → T\n"
     ]
    }
   ],
   "source": [
    "# Print the policy learned from the Q-table\n",
    "\n",
    "policy = np.argmax(Q, axis=1)\n",
    "arrows = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\"}\n",
    "\n",
    "for r in range(env.n_rows):\n",
    "    row = []\n",
    "    for c in range(env.n_cols):\n",
    "        s = r * env.n_cols + c\n",
    "        row.append(\"T\" if s == env.terminal_s else arrows[policy[s]])\n",
    "    print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177d5328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA policy   : {'episodes': 200, 'success_rate': 1.0, 'avg_steps': 6.0, 'avg_reward': 0.9499999999999998, 'min_steps': 6, 'max_steps': 6}\n",
      "Random baseline: {'episodes': 200, 'success_rate': 0.985, 'avg_steps': 54.35, 'avg_reward': 0.4513499999999997, 'min_steps': 7, 'max_steps': 200}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(env, Q, n_episodes=200, max_steps=200):\n",
    "    \"\"\"\n",
    "    Evaluate the policy induced by Q over many episodes.\n",
    "    Returns metrics in a dict\n",
    "    \"\"\"\n",
    "    wins, steps_list, rewards = 0, [], []\n",
    "    for _ in range(n_episodes):\n",
    "        s, _ = env.reset()\n",
    "        total_reward, steps = 0.0, 0\n",
    "        done = False\n",
    "        last_terminated = False\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            a = int(np.argmax(Q[s]))        \n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "            last_terminated = terminated\n",
    "\n",
    "        wins += int(last_terminated)\n",
    "        steps_list.append(steps)\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return {\n",
    "        \"episodes\": int(n_episodes),\n",
    "        \"success_rate\": wins / n_episodes,\n",
    "        \"avg_steps\": float(np.mean(steps_list)),\n",
    "        \"avg_reward\": float(np.mean(rewards)),\n",
    "        \"min_steps\": int(np.min(steps_list)),\n",
    "        \"max_steps\": int(np.max(steps_list)),\n",
    "    }\n",
    "\n",
    "\n",
    "def random_baseline(env, n_episodes=200, max_steps=200):\n",
    "    \"\"\"\n",
    "    Baseline that acts uniformly at random each step.\n",
    "    Returns metrics in a dict\n",
    "    \"\"\"\n",
    "    wins, steps_list, rewards = 0, [], []\n",
    "    for _ in range(n_episodes):\n",
    "        s, _ = env.reset()\n",
    "        total_reward, steps = 0.0, 0\n",
    "        done = False\n",
    "        last_terminated = False\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            a = env.action_space.sample()        \n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "            last_terminated = terminated\n",
    "\n",
    "        wins += int(last_terminated)\n",
    "        steps_list.append(steps)\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return {\n",
    "        \"episodes\": int(n_episodes),\n",
    "        \"success_rate\": wins / n_episodes,\n",
    "        \"avg_steps\": float(np.mean(steps_list)),\n",
    "        \"avg_reward\": float(np.mean(rewards)),\n",
    "        \"min_steps\": int(np.min(steps_list)),\n",
    "        \"max_steps\": int(np.max(steps_list)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare SARSA vs Random Actions\n",
    "sarsa_metrics  = evaluate_policy(GridworldEnv(4, 4), Q, n_episodes=200, max_steps=200)\n",
    "random_metrics = random_baseline(GridworldEnv(4, 4), n_episodes=200, max_steps=200)\n",
    "print(\"SARSA policy   :\", sarsa_metrics)\n",
    "print(\"Random baseline:\", random_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f4608ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . . \n",
      ". . . . \n",
      ". . . . \n",
      ". . . T  \n",
      "\n",
      ". . . . \n",
      "A . . . \n",
      ". . . . \n",
      ". . . T  \n",
      "\n",
      ". . . . \n",
      ". A . . \n",
      ". . . . \n",
      ". . . T  \n",
      "\n",
      ". . . . \n",
      ". . . . \n",
      ". A . . \n",
      ". . . T  \n",
      "\n",
      ". . . . \n",
      ". . . . \n",
      ". . A . \n",
      ". . . T  \n",
      "\n",
      ". . . . \n",
      ". . . . \n",
      ". . . A \n",
      ". . . T  \n",
      "\n",
      ". . . . \n",
      ". . . . \n",
      ". . . . \n",
      ". . . A  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##  Visualizing the Agent's path through the grid using the learned policy\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "# Show the starting position\n",
    "env.draw()\n",
    "\n",
    "# Run one episode following the greedy policy (no exploration)\n",
    "while not done and steps < 50:  # 50-step safety cap\n",
    "    # Choose best action for current state\n",
    "    action = np.argmax(Q[state])\n",
    "\n",
    "    # Take the action\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Show updated grid\n",
    "    env.draw()\n",
    "\n",
    "    # Update state and step count\n",
    "    state = next_state\n",
    "    steps += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
